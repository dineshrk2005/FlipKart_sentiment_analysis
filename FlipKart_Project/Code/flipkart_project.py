# -*- coding: utf-8 -*-
"""FlipKart_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o1Uox6eFxgdpI8RMTzr4SXZDoDpL1ww
"""

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# --- Download NLTK data (if not already downloaded) ---
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')

# --- 1. Data Loading and Preprocessing ---

# Load the dataset
try:
    df = pd.read_csv(r"C:\Users\Asus\OneDrive\Desktop\FlipKart_Project\Dataset.csv", encoding='latin1', low_memory=False)
except FileNotFoundError:
    print("Error: Dataset.csv not found. Make sure the file is in the correct directory.")
    exit()


# --- Data Cleaning for Sentiment Analysis ---

# Function to clean the text data
def clean_text(text):
    if not isinstance(text, str):
        return ""
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Convert to lowercase
    text = text.lower()
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Remove stopwords and perform stemming
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    words = text.split()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return " ".join(words)

# Apply the cleaning function to the 'Review' and 'Summary' columns
df['cleaned_review'] = df['Review'].apply(clean_text)
df['cleaned_summary'] = df['Summary'].apply(clean_text)

# Combine the cleaned review and summary into a single text column
df['combined_text'] = df['cleaned_review'] + ' ' + df['cleaned_summary']


# Convert 'Rate' to numeric, coercing errors
df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')
df.dropna(subset=['Rate'], inplace=True) # Drop rows where 'Rate' could not be converted
df['Rate'] = df['Rate'].astype(int) # Convert to integer

# Create a 'sentiment' column based on the 'Rate' column
def map_sentiment(rate):
    if rate >= 4:
        return 'positive'
    elif rate == 3:
        return 'neutral'
    else:
        return 'negative'

df['sentiment'] = df['Rate'].apply(map_sentiment)

# --- 2. Chart Generation ---

# Clean the 'Price' column for chart generation
def clean_price(price):
    if isinstance(price, str):
        # Remove currency symbols and commas
        price = price.replace('Ã¼??', '').replace(',', '')
    return pd.to_numeric(price, errors='coerce')

df['Price'] = df['Price'].apply(clean_price)
df_charts = df.dropna(subset=['Price']).copy() # Create a copy for charting to avoid affecting the main df

# 1. Distribution of Product Prices
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df_charts['Price'], bins=30, kde=True)
plt.title('Distribution of Product Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.subplot(1, 2, 2)
sns.boxplot(x=df_charts['Price'])
plt.title('Box Plot of Product Prices')
plt.xlabel('Price')
plt.tight_layout()
plt.show()

# 2. Top 10 Most Reviewed Products
top_products = df['Product_name'].value_counts().nlargest(10)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_products.values, y=top_products.index, palette='viridis')
plt.title('Top 10 Most Reviewed Products')
plt.xlabel('Number of Reviews')
plt.ylabel('Product Name')
plt.tight_layout()
plt.show()

# 3. Average Rating for Top 10 Products
top_product_names = top_products.index
avg_ratings = df[df['Product_name'].isin(top_product_names)].groupby('Product_name')['Rate'].mean().sort_values(ascending=False)
plt.figure(figsize=(12, 8))
sns.barplot(x=avg_ratings.values, y=avg_ratings.index, palette='plasma')
plt.title('Average Rating for Top 10 Most Reviewed Products')
plt.xlabel('Average Rating')
plt.ylabel('Product Name')
plt.xlim(0, 5)
plt.tight_layout()
plt.show()

# 4. Price vs. Rating Scatter Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Price', y='Rate', data=df_charts, alpha=0.5)
plt.title('Price vs. Rating')
plt.xlabel('Price')
plt.ylabel('Rating')
plt.show()

# 5. Word Clouds for Sentiments
positive_text = " ".join(df[df['sentiment'] == 'positive']['combined_text'])
negative_text = " ".join(df[df['sentiment'] == 'negative']['combined_text'])

positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
negative_wordcloud = WordCloud(width=800, height=400, background_color='black').generate(negative_text)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.title('Most Frequent Words in Positive Reviews')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.title('Most Frequent Words in Negative Reviews')
plt.axis('off')
plt.show()

# --- 3. Feature Extraction ---

# Use TF-IDF to convert text data into numerical vectors
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X = tfidf_vectorizer.fit_transform(df['combined_text'])
y = df['sentiment']

# --- 4. Model Building ---

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Logistic Regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# --- 5. Model Evaluation ---

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Display the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred, labels=['negative', 'neutral', 'positive'])
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['negative', 'neutral', 'positive'],
            yticklabels=['negative', 'neutral', 'positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# --- 6. Prediction on New Data ---

# Function to predict the sentiment of a new review
def predict_sentiment(review_text):
    cleaned_text = clean_text(review_text)
    vectorized_text = tfidf_vectorizer.transform([cleaned_text])
    prediction = model.predict(vectorized_text)
    return prediction[0]

# Example of predicting the sentiment of a new review
new_review = "This product is amazing! I love it."
predicted_sentiment = predict_sentiment(new_review)
print(f"\nThe sentiment of the new review is: {predicted_sentiment}")

new_review_2 = "This is the worst product I have ever bought."
predicted_sentiment_2 = predict_sentiment(new_review_2)
print(f"The sentiment of the second new review is: {predicted_sentiment_2}")